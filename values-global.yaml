---
global:
  pattern: opea-chatqna-on-xeon
  options:
    useCSV: false
    syncPolicy: Automatic
    installPlanApproval: Automatic

  xeonllm:
    namespace: xeon-llm
    build_envs: [] 
    runtime_envs: []

    tei_embedding:
      image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
      model_id: BAAI/bge-large-en-v1.5 # change to set custom TEI embedding model
#   uncomment to customize PVC size if using TEI models bigger thanBAAI/bge-large-en-v1.5
#      pvc:i
#        size: 10Gi
    tei_xeon:
      image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
      model_id: BAAI/bge-reranker-large # change to set custom TEI reranker model on CPU
#      pvc:
#        size: 10Gi
#    TODO: IMPLEMENT CUSTOMIZING TGI ENDPOINT FROM RHOAI

    servingRuntime:
      namespace: xeon-llm
      name: tgi-70b-xeon

    chatqna_xeon_backend: 
      image: chatqna:latest
    chatqna_ui_server:  
      image: chatqna-ui:latest
    dataprep:  
      image: dataprep-redis:latest
    embedding:  
      image: embedding-tei:latest
    xeonllm_init: {}
    llm_server_for_xeon:  
      image: llm-tgi:latest
    redis_vector_db:  
      image: redis/redis-stack:7.2.0-v9
    reranking:  
      image: reranking-tei:latest
    retriever:
      image: retriever-redis:latest
  
main:
  clusterGroupName: hub
  multiSourceConfig:
    enabled: true

