global: 
  pattern: xeonllm

  xeonllm:
    namespace: xeon-llm
    build_envs: [] # http_proxy/https_prxy can be set here
    runtime_envs: []

    tei_embedding:
      git_ref: a7ee6f1fe13ce8aed440d0baa0953cd8f5c23e0a
      image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
      pvc:
        size: 3Gi
      model_id: BAAI/bge-base-en-v1.5
